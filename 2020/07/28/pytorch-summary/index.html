<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="PyTorch," />










<meta name="description" content="Intro  table th:nth-of-type(1){ width: 20%; } table th:nth-of-type(2){ width: 80% ; }      Package Description    torch The top-level PyTorch package and tensor library.   torch.nn A subpackage that c">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-summary">
<meta property="og:url" content="http://yoursite.com/2020/07/28/pytorch-summary/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Intro  table th:nth-of-type(1){ width: 20%; } table th:nth-of-type(2){ width: 80% ; }      Package Description    torch The top-level PyTorch package and tensor library.   torch.nn A subpackage that c">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="file:///Users/zhengxing/Screenshots/pytorch-list-of-modules-with-intro.png">
<meta property="article:published_time" content="2020-07-28T02:11:29.000Z">
<meta property="article:modified_time" content="2020-10-08T16:32:27.608Z">
<meta property="article:author" content="Zheng Xing">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="file:///Users/zhengxing/Screenshots/pytorch-list-of-modules-with-intro.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/07/28/pytorch-summary/"/>





  <title>pytorch-summary | Hexo</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/28/pytorch-summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zheng Xing">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">pytorch-summary</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-07-28T10:11:29+08:00">
                2020-07-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/07/28/pytorch-summary/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/07/28/pytorch-summary/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p><img src="file:///Users/zhengxing/Screenshots/pytorch-list-of-modules-with-intro.png" alt=""></p>
<style>
table th:nth-of-type(1){
width: 20%;
}
table th:nth-of-type(2){
width: 80%
;
}
</style>

<table>
<thead>
<tr>
<th>Package</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>torch</td>
<td>The top-level PyTorch package and tensor library.</td>
</tr>
<tr>
<td>torch.nn</td>
<td>A subpackage that contains modules and extensible classes for building neural networks.</td>
</tr>
<tr>
<td>torch.autograd</td>
<td>A subpackage that supports all the differentiable Tensor operations in PyTorch.</td>
</tr>
<tr>
<td>torch.nn.functional</td>
<td>A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations.</td>
</tr>
<tr>
<td>torch.optim</td>
<td>A subpackage that contains standard optimization operations like SGD and Adam.</td>
</tr>
<tr>
<td>torch.utils</td>
<td>A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.</td>
</tr>
<tr>
<td>torchvision</td>
<td>A package that provides access to popular datasets, model architectures, and image transformations for computer vision.</td>
</tr>
</tbody></table>
<h1 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a>Tensor 操作</h1><h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 列表转 tensor</span></span><br><span class="line">aa = [ [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>] ]</span><br><span class="line">t = torch.tensor(aa)    <span class="comment"># t 的数据类型是 torch.Int32, 此处用了 factory 方式，it will infer the datatype</span></span><br><span class="line">t = torch.Tensor(aa)    <span class="comment"># t 的数据类型是 torch.Float32, 因为这里是调用了 constructor, 使用了全局的默认的数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. numpy 向量转 tensor</span></span><br><span class="line">aa = np.array([ [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>] ])</span><br><span class="line">t = torch.from_numpy(aa)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 利用大写接受 shape 创建</span></span><br><span class="line">torch.empty(<span class="number">2</span>, <span class="number">3</span>)    <span class="comment"># 生成一个 2x3 的 0 矩阵</span></span><br><span class="line">torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)    <span class="comment"># 生成一个 2x3 的随机矩阵</span></span><br><span class="line">torch.IntTensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.FloatTensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 随机创建 Tensor</span></span><br><span class="line">a=torch.rand(<span class="number">3</span>,<span class="number">3</span>)    <span class="comment">#创建3*3的0到1均匀分布的矩阵</span></span><br><span class="line">a=torch.randn(<span class="number">3</span>,<span class="number">3</span>)    <span class="comment">#均值为0方差为1正态分布矩阵</span></span><br><span class="line"></span><br><span class="line">torch.rand_like(a)    <span class="comment">#等价于下一条</span></span><br><span class="line">torch.rand(a.shape)</span><br><span class="line"></span><br><span class="line">torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">3</span>,<span class="number">3</span>])<span class="comment">#创建3*3的1到10随机分布的整数矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 创建相同数的矩阵：</span></span><br><span class="line">torch.full([<span class="number">3</span>,<span class="number">3</span>],<span class="number">1</span>)    <span class="comment"># 生成3*3的全为1的矩阵</span></span><br><span class="line">torch.full([],<span class="number">1</span>)    <span class="comment"># 生成标量1</span></span><br><span class="line">torch.full([<span class="number">2</span>],<span class="number">1</span>)    <span class="comment"># 生成一个长度为2的值全为1的向量</span></span><br><span class="line">torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">torch.zeros(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">torch.eye(<span class="number">3</span>,<span class="number">4</span>)    <span class="comment"># 生成对角为1矩阵，若不是对角矩阵，则多余出用0填充</span></span><br><span class="line">torch.eye(<span class="number">3</span>)    <span class="comment"># 3*3对角矩阵</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 创建规律数列矩阵：</span></span><br><span class="line">torch.arange(<span class="number">0</span>,<span class="number">10</span>)    <span class="comment"># 生成[0,1,2,3,4,5,6,7,8,9]</span></span><br><span class="line">torch.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">2</span>)    <span class="comment"># 生成增量为2的数列</span></span><br><span class="line">torch.arange(<span class="number">10</span>)    <span class="comment"># 效果同于（0，10）</span></span><br><span class="line"></span><br><span class="line">torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">4</span>)    <span class="comment"># 生成[0.0000,3.3333,6,6667,10,0000]包括10的4等分向量</span></span><br><span class="line">torch.logspace(<span class="number">0</span>,<span class="number">-1</span>,steps=<span class="number">10</span>,base=<span class="number">10</span>)    <span class="comment"># 生成10个0到-1等分的数，再以其为指数，如第一个数#为1.000,最后一个数为0.100</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># 7.创建随机打散数组：</span></span><br><span class="line">torch.randperm(<span class="number">10</span>)    <span class="comment"># 生成0~9这10个数乱序的数组（常用作索引）</span></span><br></pre></td></tr></table></figure>

<p><mark>注意四种方式的区别</mark></p>
<ol>
<li>data = np.array([1, 2, 3])</li>
<li>四种方式<ol>
<li>t1 = torch.Tensor(data)    # dtype=Torch.float32, 改变 data 不会影响 t1 的值</li>
<li>t2 = torch.tensor(data)    # dtype=Torch.int32， 改变 data 不会影响 t2 的值</li>
<li>t3 = torch.as_tensor(data)    # dtype=Torch.int32，改变 data 会反映到 t3 里面</li>
<li>t4 = torch.from_numpy(data)    # dtype=Torch.int32， 改变 data 会反映到 t4 里面。</li>
</ol>
</li>
<li>因此后两种是 <font color='red'> zero memory-copy </font>，其创建效率会高一点！</li>
<li>实际中，第一常用的是 torch.tensor, 第二常用的是 torch.as_tensor().</li>
</ol>
<h2 id="分析性质"><a href="#分析性质" class="headerlink" title="分析性质"></a>分析性质</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aa = [ [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>] ]</span><br><span class="line">t = torch.tensor(aa)</span><br><span class="line"></span><br><span class="line">type(t)    <span class="comment"># torch.Tensor</span></span><br><span class="line">t.shape    <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"></span><br><span class="line">t.reshape(<span class="number">1</span>, <span class="number">9</span>)    <span class="comment"># tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])</span></span><br><span class="line">t.reshape(<span class="number">1</span>, <span class="number">9</span>).shape    <span class="comment"># torch.Size([1, 9])</span></span><br></pre></td></tr></table></figure>

<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(a,b)    <span class="comment"># 返回对比后的矩阵，若矩阵形状相同，那么会对没一个位置进行比较，返回一个同样形状的矩阵，相应位置若相同则返回1，不相等则为0</span></span><br><span class="line">torch.all(torch.eq(a,b))    <span class="comment"># a,b相同时返回1，否则为1。即all内的张量为全1矩阵会返回1</span></span><br></pre></td></tr></table></figure>

<h2 id="切片-start-end-step"><a href="#切片-start-end-step" class="headerlink" title="切片(start : end : step)"></a>切片(start : end : step)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">img=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)    <span class="comment"># 4张图片</span></span><br><span class="line"></span><br><span class="line">img[<span class="number">1</span>]    <span class="comment"># 获取第二张图片</span></span><br><span class="line">img[<span class="number">0</span>,<span class="number">0</span>].shape    <span class="comment"># 获取第一张图片的第一个通道的图片形状</span></span><br><span class="line">img[<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>]    <span class="comment"># 返回像素灰度值标量</span></span><br><span class="line"></span><br><span class="line">img[:<span class="number">2</span>]    <span class="comment"># 获得img[0]和img[1]</span></span><br><span class="line"><span class="comment">#img[:2,:1]==img[:2,:1,:,:]</span></span><br><span class="line">img[<span class="number">2</span>:]    <span class="comment"># 获得img[2],img[3],img[4]三张图片</span></span><br><span class="line">img[<span class="number">-1</span>:]    <span class="comment"># 获得img[4]</span></span><br><span class="line">img[:,:,::<span class="number">2</span>,::<span class="number">2</span>]    <span class="comment"># 对图片进行隔行（列）采样</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#还有一种索引中的...操作，有自动填充的功能,一般用于维数很多时使用。</span></span><br><span class="line">img[<span class="number">0</span>,...]    <span class="comment"># img.shape的结果是torch.Size([4,28,28]),这是和img[0,:]或者img[0]是一样的。</span></span><br><span class="line">img[<span class="number">0</span>,...,<span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>]    <span class="comment"># 此时由于写了最右边的索引，中间的...等价于:,:，即img[0,:,:,0:28:2]</span></span><br></pre></td></tr></table></figure>

<h2 id="索引查找"><a href="#索引查找" class="headerlink" title="索引查找"></a>索引查找</h2><p>这里介绍 index_select/masked_select/take</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#同样是上面的img</span></span><br><span class="line"><span class="comment">#比如要取前三张图片，那么就是针对第一个维度（图片数目）进行挑选</span></span><br><span class="line"></span><br><span class="line">img.index_select(<span class="number">0</span>,torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]))    <span class="comment"># 第一个参数为轴，第二个参数为tensor类型的索引</span></span><br><span class="line">img.index_select(<span class="number">0</span>,torch.arange(<span class="number">3</span>))    <span class="comment"># 效果同上句</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用掩码mask</span></span><br><span class="line">x=torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">mask=x.ge(<span class="number">0.5</span>)    <span class="comment"># 会把x中大于0.5的置为一，其他置为0，类似于阈值化操作。</span></span><br><span class="line">y=torch.masked_select(x,mask)    <span class="comment"># 将mask中值为1的元素取出来，比如mask有3个位置值为1</span></span><br><span class="line">y.shape    <span class="comment"># 结果为tensor.Size([3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用take取元素</span></span><br><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">torch.take(x,torch.tensor([<span class="number">0</span>,<span class="number">2</span>,<span class="number">6</span>]))</span><br><span class="line"><span class="comment">#则最后结果为tensor([1,3,6]),也就是说会先将tensor压缩成一维向量，再按照索引取元素。</span></span><br></pre></td></tr></table></figure>


<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p><mark>！！！这里非常重要！！！</mark></p>
<h3 id="view-reshape-flatten"><a href="#view-reshape-flatten" class="headerlink" title="view/reshape/flatten"></a>view/reshape/flatten</h3><p>view 和 reshape 的作用是一毛一样的！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">x=img.view(<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">img.view(<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">x.view(<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)<span class="comment">#此操作可行但不合理，逻辑上的问题会造成信息污染</span></span><br></pre></td></tr></table></figure>

<p>flatten 的话很常用！比如一般情况下，在 fully connected layer 层之前，我们一般会把除了 batch 那个维度以外的部分给 flatten 掉。此时可以用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">3</span>,<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">t.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然，也可以用 reshape 来实现</span></span><br><span class="line">t.reshape(t.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="squeeze-unsqueeze"><a href="#squeeze-unsqueeze" class="headerlink" title="squeeze/unsqueeze"></a>squeeze/unsqueeze</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## squeeze 减少维度数, unsqueeze 扩展维度数 ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># squeeze只关心有值的，可以挤压该值只有1个的维度，则最后会保留值总数目</span></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">32</span>,<span class="number">1</span>,<span class="number">1</span>)    <span class="comment"># 1张图，有32个通道，每个通道一个像素</span></span><br><span class="line">x.squeeze()    <span class="comment"># 形状变为[32]</span></span><br><span class="line">x.squeeze(<span class="number">0</span>)    <span class="comment"># [32,1,1]</span></span><br><span class="line">x.squeeze(<span class="number">1</span>)    <span class="comment"># [1,32,1,1], 并没有发生压缩，因为该维度上有值，不能减少这一维度，不会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># unsqueeze会在参数维度上进行，若有此维度则会先将当前维度后移再拓展</span></span><br><span class="line">img = torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">img.unsqueeze(<span class="number">0</span>).shape    <span class="comment"># 结果为torch.Size([1,4,1,28,28]),物理意义为batch</span></span><br><span class="line">img.unsqueeze(<span class="number">-1</span>).shape    <span class="comment"># 结果为torch.Size([4,1,28,28,1]),等价于unsqueeze(4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 例如图片要在某个维度上面做加减，那么仅仅有一维数据是不够的，此时就需要将一维数据扩展维度</span></span><br><span class="line">b = torch.rand(<span class="number">32</span>)    <span class="comment"># torch.Size([32])</span></span><br><span class="line">f = torch.rand(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)    <span class="comment"># 要做到在每个channel上增加某一bias</span></span><br><span class="line">b = b.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">0</span>)    <span class="comment"># 形状为[1,32,1,1]</span></span><br><span class="line"><span class="comment"># 但仅仅如此是不够的，我们知道矩阵进行加减操作是需要形状完全一致的，所以形状必须为[4,32,14,14],这就需要在某一维度上进行拓展的操作，需要了解后面的api(expand)才可以解决。</span></span><br></pre></td></tr></table></figure>

<h3 id="expand-repeat"><a href="#expand-repeat" class="headerlink" title="expand/repeat"></a>expand/repeat</h3><p>expand 操作返回 tensor 的一个新视图，单个维度扩大为更大的尺寸。tensor也可以扩大为更高维，新增加的维度将附在前面。 <mark>扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。</mark>任何一个一维的在不分配新内存情况下可扩展为任意的数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## expand 仅在有需要时增加数据的扩展(starstarstar)/repeat主动增加数据的扩展##</span></span><br><span class="line"><span class="comment">#为了将[1,32,1,1]扩展为[4,32,14,14]:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># expand</span></span><br><span class="line"><span class="comment"># 这里接上面的代码，此时 b 的形状是 [1, 32, 1, 1]</span></span><br><span class="line">b.expend(<span class="number">4</span>,<span class="number">32</span>,<span class="number">14</span>,<span class="number">14</span>)    <span class="comment"># 直接输入想要的形状，但是只有原维度上的数值为1时才可以进行扩展</span></span><br><span class="line">b.expend(<span class="number">4</span>,<span class="number">33</span>,<span class="number">14</span>,<span class="number">14</span>)    <span class="comment"># 报错</span></span><br><span class="line">b.expend(<span class="number">4</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>)    <span class="comment"># -1 所在的维度不变，仅怎加第0维的内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># repeat，注意参数输入的是在该维度上拷贝的次数而不是形状！</span></span><br><span class="line">b.repeat(<span class="number">4</span>,<span class="number">32</span>,<span class="number">1</span>,<span class="number">1</span>)    <span class="comment"># [4,1024,1,1] 1024=32*32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正确操作</span></span><br><span class="line">b.repeat(<span class="number">4</span>,<span class="number">1</span>,<span class="number">14</span>,<span class="number">14</span>)    <span class="comment"># [4,32,14,14]</span></span><br></pre></td></tr></table></figure>

<h3 id="transpose-permute"><a href="#transpose-permute" class="headerlink" title="transpose/permute"></a>transpose/permute</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于二维向量来说：</span></span><br><span class="line">a = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">a.t()    <span class="comment"># 即完成了转置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于任意维度张量的转置操作：transpose</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">a.transpose(<span class="number">1</span>,<span class="number">3</span>)    <span class="comment"># 接收参数为要进行交换那两个维度，[4,32,32,3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接收index来进行转置：permute</span></span><br><span class="line">a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>)    <span class="comment"># 形状为[4,32,32,3]</span></span><br></pre></td></tr></table></figure>

<h2 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting:"></a>Broadcasting:</h2><p>在矩阵相加时，如果两矩阵的形状不一致，则会自动运行broadcast</p>
<ol>
<li>有点类似于先 unsqueeze 再 expand</li>
<li>自动会在第 0 维处插入一个维度,并且同时将形状为1的部分自动转换成想要运算的对象的形状，这表示着在 broadcast 前先要将后面的维度数调至想要的样子再使用。</li>
<li>例如，将形状为(32)的向量转化为 (4,32,14,14), 则先要 unsueeze 到(32,1,1), 才会自动 broadcast为(4,32,14,14).</li>
<li>转换成物理意义容易理解,比如矩阵加上一个数就是 broadcast.比如上面为图片数据时,(32)就是对32个通道想要加的不同bias，会自动广播为对每个灰度值上的bias。（真正运算的是该数据的最基本单位）。加上（14，14）就是对每一张图的每个通道的图上加上一个(14,14)的bias。</li>
</ol>
<h2 id="拼接与拆分-cat-stack-spilit-chunk"><a href="#拼接与拆分-cat-stack-spilit-chunk" class="headerlink" title="拼接与拆分(cat/stack/spilit/chunk):"></a>拼接与拆分(cat/stack/spilit/chunk):</h2><p>可以参考<a href="https://deeplizard.com/learn/video/kF2AlpykJGY" target="_blank" rel="noopener">这里</a>去对比 Pytorch, TensorFlow, 和 Numpy 框架下这几个操作的语法区别。<br>特别的，TF 里面的 expand_dims 类似于 PyTorch 的 unsqueeze, PyTorch 下的 dims= 参数类似于 TF 的 axis= 参数。</p>
<h3 id="cat-拼接"><a href="#cat-拼接" class="headerlink" title="cat 拼接"></a>cat 拼接</h3><p><font color='red'>torch.cat(seq, dim=0, out=None)</font>：按照已经存在的维度进行concatenate。</p>
<p>在指定的维度 dim 上对序列 seq 进行连接操作。例如：</p>
<p>参数：</p>
<ol>
<li>seq (sequence of Tensors) - Python序列或相同类型的张量序列</li>
<li>dim (int, optional) - 沿着此维度连接张量</li>
<li>out (Tensor, optional) - 输出参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">18</span>,<span class="number">18</span>)</span><br><span class="line">b = torch.rand(<span class="number">5</span>,<span class="number">3</span>,<span class="number">18</span>,<span class="number">18</span>)</span><br><span class="line">c = torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">18</span>,<span class="number">18</span>)</span><br><span class="line">d = a.copy()</span><br><span class="line"></span><br><span class="line">torch.cat([a,b],dim=<span class="number">0</span>)<span class="comment">#拼接得到(9,3,18,18)的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#若为2维数据，dim=0 则是竖向拼接，dim=0 就是横向拼接。dim所指维度可以不同，但其他维度形状必须一致</span></span><br><span class="line">torch.cat([a,c],dim=<span class="number">1</span>)    <span class="comment"># 就会得到(4,4,18,18)的数据。</span></span><br></pre></td></tr></table></figure>

<h3 id="stack增维度拼接"><a href="#stack增维度拼接" class="headerlink" title="stack增维度拼接"></a>stack增维度拼接</h3><p><mark>这个不是很好理解！当 dim=0 的时候还好，其他情况下难理解！</mark></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack([a,b],dim=<span class="number">0</span>) <span class="comment"># 得到形状为(2,4,3,18,18)。使用时列表内对象的形状需要一致。</span></span><br></pre></td></tr></table></figure>

<p>a = torch.IntTensor([[1,2,3],[11,22,33]])</p>
<p>b = torch.IntTensor([[4,5,6],[44,55,66]])</p>
<p>c = torch.stack([a,b],0)</p>
<p>d = torch.stack([a,b],1)</p>
<p>e = torch.stack([a,b],2)</p>
<p>c ：tensor([[[ 1,  2,  3],</p>
<pre><code> [11, 22, 33]],

[[ 4,  5,  6],

 [44, 55, 66]]], dtype=torch.int32)</code></pre><p>d ：tensor([[[ 1,  2,  3],</p>
<pre><code> [ 4,  5,  6]],

[[11, 22, 33],

 [44, 55, 66]]], dtype=torch.int32)</code></pre><p>e ：tensor([[[ 1,  4],</p>
<pre><code> [ 2,  5],

 [ 3,  6]],

[[11, 44],

 [22, 55],

 [33, 66]]], dtype=torch.int32)</code></pre><p>c, dim = 0时</p>
<p>c = [ a, b]</p>
<p>d, dim =1 时</p>
<p>d = [ [a[0] , b[0] ] , [a[1], b[1] ] ]</p>
<p>e, dim = 2 时</p>
<p>e=[[[a[0][0],b[0][0]],[a[0][1],b[0][1]],[a[0][2],b[0][2]]],[[a[1][0],b[1][0]],[a[1][1],b[0][1]],[a[1][2],b[1][2]]]]</p>
<h3 id="split拆分"><a href="#split拆分" class="headerlink" title="split拆分"></a>split拆分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据欲拆分长度：</span></span><br><span class="line">a1,a2 = a.split(<span class="number">2</span>,dim=<span class="number">0</span>)    <span class="comment"># 拆分长度为2.对第0维按照2个一份进行拆分。拆分获得两个形状为(2,3,18,18)的张量。</span></span><br><span class="line">a1,a2 = a.split([<span class="number">3</span>,<span class="number">1</span>],dim=<span class="number">0</span>)    <span class="comment"># 不同长度拆分。获得(3,3,18,18)和(1,3,18,18)两个形状的张量。</span></span><br></pre></td></tr></table></figure>

<h3 id="chunk拆分"><a href="#chunk拆分" class="headerlink" title="chunk拆分"></a>chunk拆分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据欲拆分数量</span></span><br><span class="line">a1,a2,a3,a4=a.chunk(<span class="number">4</span>,dim=<span class="number">0</span>)    <span class="comment"># 将张量依第0维拆分成4个(1,3,18,18)的张量。等效于a.split(1,dim=0)</span></span><br></pre></td></tr></table></figure>

<h2 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># + 等价于 torch.add()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法</span></span><br><span class="line"><span class="comment"># tensor 直接用 * 运算符的话, 其结果为 element wise, 矩阵乘法为 torch.matmul 或者 @</span></span><br><span class="line"><span class="comment"># 高维tensor矩阵相乘实际上是对多个二维矩阵进行并行运算</span></span><br><span class="line">a=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">64</span>)</span><br><span class="line">b=torch.rand(<span class="number">4</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">a@b    <span class="comment"># 结果得到的形状为(4,3,28,32)</span></span><br><span class="line"><span class="comment"># 若(4,3,28,64)@(4,1,64,32) 则会自动调用广播机制，把 (4,1,64,32) 转变为 (4,3,64,32) 再相乘。</span></span><br><span class="line"><span class="comment"># 无法调用广播机制的乘法则会报错。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方</span></span><br><span class="line">a = torch.full([<span class="number">2</span>,<span class="number">2</span>],<span class="number">2</span>) <span class="comment"># 创建一个(2,2)的全2矩阵</span></span><br><span class="line">a.pow(<span class="number">2</span>)    <span class="comment"># a的每个元素都平方</span></span><br><span class="line">a**<span class="number">2</span>    <span class="comment"># 等价于上一句</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方</span></span><br><span class="line">a.sqrt()    <span class="comment"># 平方根</span></span><br><span class="line">a**<span class="number">0.5</span>    <span class="comment"># 等价于上一句</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># exp,log</span></span><br><span class="line">a.torch.exp(torch.ones(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">torch.log(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似</span></span><br><span class="line"><span class="comment"># floor()向下取整，ceil()向上取整，round()四舍五入。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取整取小数</span></span><br><span class="line"><span class="comment"># trunc()取整，frac()取小数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clamp取范围</span></span><br><span class="line">a = torch.tensor([[<span class="number">3</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">8</span>]])</span><br><span class="line">a.clamp(<span class="number">6</span>)    <span class="comment"># 得到[[6,6],[6,8]],小于6的都变为6</span></span><br><span class="line">a.clamp(<span class="number">5</span>,<span class="number">6</span>)    <span class="comment"># 得到[[5,5],[6,6]],小于下限变为下限，大于上限变为上限。</span></span><br></pre></td></tr></table></figure>

<h2 id="统计属性"><a href="#统计属性" class="headerlink" title="统计属性"></a>统计属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 范数</span></span><br><span class="line"><span class="comment"># 求多少 p 范数只需要在 norm(p) 的参数中修改 p 即可</span></span><br><span class="line">a.norm(<span class="number">1</span>)    <span class="comment"># 求a的一范数，范数也可以加dim=</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求最大值和最小值与其相关的索引</span></span><br><span class="line">a.min()</span><br><span class="line">a.max()</span><br><span class="line">a.argmax()    <span class="comment"># 会得到索引值，返回的永远是一个标量，多维张量会先拉成向量再求得其索引。拉伸的过程为每一行加起来变成一整行，而不是matlab中的列拉成一整列。</span></span><br><span class="line">a.argmin()</span><br><span class="line">a.argmax(dim=<span class="number">1</span>)    <span class="comment"># 如果不想获取拉伸后的索引值就需要在指定维度上进行argmax，比如如果a为(2，2)的矩阵，那么这句话的结果就可能是[1,1],表示第一行第一个在此行最大，第二行第一个在此行最大。</span></span><br><span class="line">a.argmax(dim=<span class="number">0</span>)    <span class="comment"># 同样的，这个会在第一个维度上去找每一列的的最大值</span></span><br><span class="line">a.max(dim=<span class="number">0</span>)    <span class="comment"># 这个会返回两个 tensor, 第一个 tensor 是每一列的最大值，第二个 tensor 是这些最大值的 index 是多少。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#累加总和</span></span><br><span class="line">a.sum()</span><br><span class="line"></span><br><span class="line"><span class="comment">#累乘综合</span></span><br><span class="line">a.prod()</span><br><span class="line"></span><br><span class="line"><span class="comment">#dim,keepdim</span></span><br><span class="line"><span class="comment">#假设a的形状为(4,10)</span></span><br><span class="line">a.max(dim=<span class="number">1</span>)<span class="comment">#结果会得到一个(4)的张量，表示4个样本中每个样本10个特征的最大值组成的张量。(max换成argmax也是同理)。</span></span><br><span class="line">a.max(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)<span class="comment">#同时返回a.argmax(dim=1)得到的结果，以保持维度数目和原来一致。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#top-k,k-th</span></span><br><span class="line">a.topk(<span class="number">5</span>)<span class="comment">#返回张量a前5个最大值组成的向量</span></span><br><span class="line">a.topk(<span class="number">5</span>,dim=<span class="number">1</span>,largest=<span class="literal">False</span>)<span class="comment">#关闭largest求最小的5个</span></span><br><span class="line">a.kthvalue(<span class="number">8</span>,dim=<span class="number">1</span>)<span class="comment">#返回第八小的值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#比较操作</span></span><br><span class="line"><span class="comment">#都是进行element-wise操作</span></span><br><span class="line">torch.eq(a,b)<span class="comment">#返回的是张量</span></span><br><span class="line">torch.equal(a,b)<span class="comment">#返回的是True或者False</span></span><br></pre></td></tr></table></figure>

<h2 id="where-gather"><a href="#where-gather" class="headerlink" title="where/gather"></a>where/gather</h2><p>函数<font color='red'>torch.gather(input, dim, index, out=None) → Tensor</font><br>沿给定轴 dim ,将输入索引张量 index 指定位置的值进行聚合.<br>对一个 3 维张量,输出可以定义为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gather</span></span><br><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure>

<p>Parameters:</p>
<ol>
<li>input (Tensor) – 源张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 聚合元素的下标(index需要是torch.longTensor类型)</li>
<li>out (Tensor, optional) – 目标张量</li>
</ol>
<p>一个应用：在动手学习深度学习中学到了一个函数gather，原文是说可以通过gather得到标签的预测概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.6</span>],[<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">0.5</span>]])</span><br><span class="line">y = torch.LongTensor([<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line">y_hat.gather(<span class="number">1</span>,y.view(<span class="number">-1</span>,<span class="number">1</span>)) </span><br><span class="line">tensor([[<span class="number">0.1000</span>],</span><br><span class="line">        [<span class="number">0.5000</span>]])</span><br></pre></td></tr></table></figure>

<p>where 函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.where(condition,x,y)</span><br></pre></td></tr></table></figure>


<h1 id="DataSet-DataLoader"><a href="#DataSet-DataLoader" class="headerlink" title="DataSet/DataLoader"></a>DataSet/DataLoader</h1><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><p>可以使用 torchvision 自带的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">'./data/FashionMNIST'</span></span><br><span class="line">    , train=<span class="literal">True</span></span><br><span class="line">    , download=<span class="literal">True</span></span><br><span class="line">    , transform=transforms.Compost([</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set)    <span class="comment"># 这里有个参数 batchsize 使用了默认值 10</span></span><br></pre></td></tr></table></figure>

<p><mark>注意 train_set 和 train_loader 不一样！！！</mark></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先看一下使用 train_set</span></span><br><span class="line">sample = next(iter(train_set))</span><br><span class="line">len(sample)    <span class="comment"># 2</span></span><br><span class="line">type(sample)    <span class="comment"># tuple</span></span><br><span class="line">image, label = sample</span><br><span class="line">image.shape    <span class="comment"># torch.Size([1, 28, 28])</span></span><br><span class="line">label.shape    <span class="comment"># torch.Size([])</span></span><br><span class="line">plt.imshow(image.squeeze(), cmp=<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再看一下 train_loader</span></span><br><span class="line">batch = next(iter(train_loader))</span><br><span class="line">len(batch)    <span class="comment"># 2</span></span><br><span class="line">type(batch)    <span class="comment"># list</span></span><br><span class="line">images, labels = batch</span><br><span class="line">images.shape    <span class="comment"># torch.Size([10, 1, 28, 28]), 这里大小是 10 因为上面在构建 DataLoader 的时候使用了默认的 "batchsize=10"</span></span><br><span class="line">labels.shape    <span class="comment"># torch.Size([10])</span></span><br></pre></td></tr></table></figure>

<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>首先需要回顾一下<a href="https://zheng-xing.github.io/2018/03/27/Advance-Python-Topics/#Scaling-with-Generators" target="_blank" rel="noopener">这里</a> 复习一下可迭代对象，迭代器，生成器的概念！</p>
<p>在使用 pytorch 训练模型，经常需要加载大量图片数据，因此 pytorch 提供了好用的数据加载工具 Dataloader。</p>
<p>为了实现小批量循环读取大型数据集，在Dataloader类具体实现中，使用了迭代器和生成器。</p>
<p>可迭代对象，迭代器，生成器 这三个概念互相关联，并不是孤立的。在可迭代对象的基础上发展了迭代器，在迭代器的基础上又发展了生成器。</p>
<p>下面的代码显示了如何如何构建自己的 Dataset 并且可以让 Dataloader 支持它</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OHLC</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file)</span>:</span></span><br><span class="line">        self.data = pd.read_csv(csv_file)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        r = self.data.iloc[index]</span><br><span class="line">        label = torch.tensor(r.is_up_data, dtype=torch.long)</span><br><span class="line">        sample = self.normalize(torch.tensor([r.open, r.high, r.low, r.close]))</span><br><span class="line">        <span class="keyword">return</span> sample, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br></pre></td></tr></table></figure>

<p>循环读数据可分为下面三种应用场景，对应着容器（可迭代对象），迭代器和生成器：</p>
<ol>
<li>for x in container: 为了遍历 python 内部序列容器（如list）, 这些类型内部实现了 <code>__getitem__()</code> 方法，可以从 0 开始按顺序遍历序列容器中的元素。</li>
<li>for x in iterator: 为了循环用户自定义的迭代器，需要实现 <code>__iter__</code> 和 <code>__next__</code>方法，<code>__iter__</code>是迭代协议，具体每次迭代的执行逻辑在 <code>__next__</code> 或 next 方法里</li>
<li>for x in generator: 为了节省循环的内存和加速，使用生成器来实现惰性加载，在迭代器的基础上加入了 <code>yield</code> 语句，最简单的例子是 range(5)</li>
</ol>
<p>pytorch 采用 for x in iterator 模式，从 Dataloader 类中读取数据。</p>
<ol>
<li>为了实现该迭代模式，在 Dataloader 内部实现 <code>__iter__</code> 方法，实际返回的是 <code>_DataLoaderIter</code> 类。</li>
<li><code>_DataLoaderIter</code> 类里面，实现了 <code>__iter__</code> 方法，返回自身，具体执行读数据的逻辑，在 <code>__next__</code> 方法中。</li>
</ol>
<p>以下代码只截取了单线程下的数据读取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">r"""</span></span><br><span class="line"><span class="string">  Data loader. Combines a dataset and a sampler, and provides</span></span><br><span class="line"><span class="string">  single- or multi-process iterators over the dataset.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dataset, batch_size=<span class="number">1</span>, shuffle=False, ...)</span>:</span></span><br><span class="line">    self.dataset = dataset</span><br><span class="line">    self.batch_sampler = batch_sampler</span><br><span class="line">    ...</span><br><span class="line">   </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> _DataLoaderIter(self)</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.batch_sampler)</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DataLoaderIter</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">r"""Iterates once over the DataLoader's dataset, as specified by the sampler"""</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, loader)</span>:</span></span><br><span class="line">    self.sample_iter = iter(self.batch_sampler)</span><br><span class="line">    ...</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.num_workers == <span class="number">0</span>: <span class="comment"># same-process loading</span></span><br><span class="line">      indices = next(self.sample_iter) <span class="comment"># may raise StopIteration</span></span><br><span class="line">      batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br><span class="line">      <span class="keyword">if</span> self.pin_memory:</span><br><span class="line">        batch = pin_memory_batch(batch)</span><br><span class="line">      <span class="keyword">return</span> batch</span><br><span class="line">    ...</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>

<p>Dataloader类中读取数据Index的方法，采用了 for x in generator 方式，但是调用采用iter和next函数</p>
<ol>
<li>构建随机采样类 RandomSampler，内部实现了 <code>__iter__</code> 方法</li>
<li><code>__iter__</code> 方法内部使用了 yield，循环遍历数据集，当数量达到 <code>batch_size</code> 大小时，就返回</li>
<li>实例化随机采样类，传入 iter 函数，返回一个迭代器</li>
<li>next 会调用随机采样类中生成器，返回相应的 index 数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomSampler</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""random sampler to yield a mini-batch of indices."""</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_size, dataset, drop_last=False)</span>:</span></span><br><span class="line">    self.dataset = dataset</span><br><span class="line">    self.batch_size = batch_size</span><br><span class="line">    self.num_imgs = len(dataset)</span><br><span class="line">    self.drop_last = drop_last</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    indices = np.random.permutation(self.num_imgs)</span><br><span class="line">    batch = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> indices:</span><br><span class="line">      batch.append(i)</span><br><span class="line">      <span class="keyword">if</span> len(batch) == self.batch_size:</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">        batch = []</span><br><span class="line">    <span class="comment">## if images not to yield a batch</span></span><br><span class="line">    <span class="keyword">if</span> len(batch)&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.drop_last:</span><br><span class="line">      <span class="keyword">yield</span> batch</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.drop_last:</span><br><span class="line">      <span class="keyword">return</span> self.num_imgs // self.batch_size</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> (self.num_imgs + self.batch_size - <span class="number">1</span>) // self.batch_size</span><br><span class="line"> </span><br><span class="line">batch_sampler = RandomSampler(batch_size. dataset)</span><br><span class="line">sample_iter = iter(batch_sampler)</span><br><span class="line">indices = next(sample_iter)</span><br></pre></td></tr></table></figure>

<p>注意点一！</p>
<ol>
<li>dataloader本质是一个可迭代对象，使用iter()访问，不能使用next()访问；</li>
<li>使用iter(dataloader)返回的是一个迭代器（iterator），然后可以使用next访问；</li>
<li>也可以使用<code>for inputs, labels in dataloaders</code>进行可迭代对象的访问；</li>
<li>一般我们实现一个datasets对象，传入到dataloader中；然后内部使用yeild返回每一次batch的数据；</li>
</ol>
<p>注意点二！</p>
<ol>
<li>DataLoader本质上就是一个iterable（跟python的内置类型list等一样），并利用多进程来加速batch data的处理，使用yield来使用有限的内存</li>
<li>Queue的特点<ol>
<li>当队列里面没有数据时： queue.get() 会阻塞， 阻塞的时候，其它进程/线程如果有queue.put() 操作，本线程/进程会被通知，然后就可以 get 成功。</li>
<li>当数据满了: queue.put() 会阻塞</li>
</ol>
</li>
<li>DataLoader是一个高效，简洁，直观的网络输入数据结构，便于使用和扩展</li>
</ol>
<h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1><h2 id="继承-nn-Module-基类"><a href="#继承-nn-Module-基类" class="headerlink" title="继承 nn.Module 基类"></a>继承 nn.Module 基类</h2><p><mark>Pytorch 中所有的层，或者网络都要继承 class Module.</mark></p>
<p>Building a neural network in PyTorch:</p>
<ol>
<li>Extend the <code>nn.Module</code> base class</li>
<li>Define layers as class attributes</li>
<li>Implement the <code>forward()</code> method </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个普通的 Network</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.layer = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        t = self.layer(t)</span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是一个 PyTorch 支持的 network</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Network, self).__init__()</span><br><span class="line">        self.layer = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        t = self.layer(t)</span><br><span class="line">        <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure>

<h2 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Network, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span>*<span class="number">4</span>*<span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="comment"># implement the forward pass</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">network.conv1.weight.shape    <span class="comment"># torch.Size([6, 1, 5, 5])</span></span><br><span class="line">network.conv2.weight.shape    <span class="comment"># torch.Size([12, 6, 5, 5])</span></span><br><span class="line">network.fc1.weight.shape      <span class="comment"># torch.Size([120, 192])</span></span><br><span class="line">network.fc2.weight.shape      <span class="comment"># torch.Size([60, 120])</span></span><br><span class="line">network.out.weight.shape      <span class="comment"># torch.Size([10, 60])</span></span><br></pre></td></tr></table></figure>

<p>或者使用 <code>.parameters()</code> 来获得所有的 parameters</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> network.parameters():</span><br><span class="line">    print(param.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#    torch.Size([6, 1, 5, 5])</span></span><br><span class="line"><span class="comment">#    torch.Size([6])</span></span><br><span class="line"><span class="comment">#    torch.Size([12, 6, 5, 5])</span></span><br><span class="line"><span class="comment">#    torch.Size([12])</span></span><br><span class="line"><span class="comment">#    torch.Size([120, 192])</span></span><br><span class="line"><span class="comment">#    torch.Size([120])</span></span><br><span class="line"><span class="comment">#    torch.Size([60, 120])</span></span><br><span class="line"><span class="comment">#    torch.Size([60])</span></span><br><span class="line"><span class="comment">#    torch.Size([10, 60])</span></span><br><span class="line"><span class="comment">#    torch.Size([10])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> network.named_parameters():</span><br><span class="line">    print(name, <span class="string">'\t\t'</span>, param.shape)</span><br></pre></td></tr></table></figure>

<p>直接按照给定的 weight matrix 去初始化一个层的 parameter</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">in_features = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">weight_matrix = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">weight_matrix.matmul(in_features)    <span class="comment"># tensor([30., 40., 50.])</span></span><br><span class="line"></span><br><span class="line">fc = nn.Linear(in_features=<span class="number">4</span>, out_features=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">fc.weight = nn.Parameter(weight_matrix)</span><br><span class="line"></span><br><span class="line">fc(in_features)    <span class="comment"># tensor([30.1672, 40.0678, 50.1432], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意，这里跟上面的结果不相等，因为这个 nn.Linear 层包含了 bias 项，如果改成下面的代码，就想通了</span></span><br><span class="line"><span class="comment"># fc = nn.Linear(in_features=4, out_features=3, bias=False)</span></span><br></pre></td></tr></table></figure>

<h2 id="forward-实现"><a href="#forward-实现" class="headerlink" title="forward 实现"></a>forward 实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">    <span class="comment"># (1) input layer</span></span><br><span class="line">    t = t</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">    t = self.conv1(t)</span><br><span class="line">    t = F.relu(t)</span><br><span class="line">    t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">    t = self.conv2(t)</span><br><span class="line">    t = F.relu(t)</span><br><span class="line">    t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">    t = t.reshape(<span class="number">-1</span>, <span class="number">12</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">    t = self.fc1(t)</span><br><span class="line">    t = F.relu(t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">    t = self.fc2(t)</span><br><span class="line">    t = F.relu(t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (6) output layer</span></span><br><span class="line">    t = self.out(t)</span><br><span class="line">    <span class="comment">#t = F.softmax(t, dim=1)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> t</span><br></pre></td></tr></table></figure>

<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="A-single-Batch"><a href="#A-single-Batch" class="headerlink" title="A single Batch"></a>A single Batch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">100</span>)</span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">batch = next(iter(train_loader)) <span class="comment"># Get Batch</span></span><br><span class="line">images, labels = batch</span><br><span class="line"></span><br><span class="line">preds = network(images)    <span class="comment"># Pass Batch</span></span><br><span class="line">loss = F.cross_entropy(preds, labels)    <span class="comment"># Calculate Loss</span></span><br><span class="line"></span><br><span class="line">loss.backward()    <span class="comment"># Calculate Gradients</span></span><br><span class="line">optimizer.step()    <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'loss1:'</span>, loss.item())</span><br><span class="line">preds = network(images)</span><br><span class="line">loss = F.cross_entropy(preds, labels)</span><br><span class="line">print(<span class="string">'loss2:'</span>, loss.item())</span><br></pre></td></tr></table></figure>

<h2 id="Many-epoch-of-many-batches"><a href="#Many-epoch-of-many-batches" class="headerlink" title="Many epoch of many batches"></a>Many epoch of many batches</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">network = Network()</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">100</span>)</span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader: <span class="comment"># Get Batch</span></span><br><span class="line">        images, labels = batch </span><br><span class="line"></span><br><span class="line">        preds = network(images) <span class="comment"># Pass Batch</span></span><br><span class="line">        loss = F.cross_entropy(preds, labels) <span class="comment"># Calculate Loss</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward() <span class="comment"># Calculate Gradients</span></span><br><span class="line">        optimizer.step() <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_correct += get_num_correct(preds, labels)</span><br><span class="line"></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">"epoch"</span>, epoch, </span><br><span class="line">        <span class="string">"total_correct:"</span>, total_correct, </span><br><span class="line">        <span class="string">"loss:"</span>, total_loss</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>


<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p>这里主要介绍一个装饰器。在做预测的时候，我们可以用如下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_preds</span><span class="params">(model, loader)</span>:</span></span><br><span class="line">    all_preds = torch.tensor([])</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> loader:</span><br><span class="line">        images, labels = batch</span><br><span class="line"></span><br><span class="line">        preds = model(images)</span><br><span class="line">        all_preds = torch.cat(</span><br><span class="line">            (all_preds, preds)</span><br><span class="line">            ,dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> all_preds</span><br></pre></td></tr></table></figure>

<p>The implantation of this function creates an empty tensor, all_preds to hold the output predictions. Then, it iterates over the batches coming from the data loader, and concatenates the output predictions with the all_preds tensor. Finally, all the predictions, all_preds, is returned to the caller.</p>
<p><mark>Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. This is because we want this functions execution to omit gradient tracking.</mark></p>
<p>This is because gradient tracking uses memory, and during inference (getting predictions while not training) there is no need to keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking feature while executing specific functions.</p>
<p>另外一种相同的方法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">10000</span>)</span><br><span class="line">    train_preds = get_all_preds(network, prediction_loader)</span><br></pre></td></tr></table></figure>

<h2 id="model-train"><a href="#model-train" class="headerlink" title="model.train()"></a>model.train()</h2><p><code>model.train()</code> 是告诉模型你在训练阶段，它并不完成具体的梯度迭代这个训练步骤哦！！！执行这个命令后，dropout 层，batchnorm 层等在训练阶段、测试阶段表现不同的这一类层会完成相应的设置！</p>
<p>当然，我们也可以调用 <code>model.eval()</code> 或者 <code>model.train(mode=False)</code> 去告诉模型我们在测试阶段！</p>
<h1 id="Analytics"><a href="#Analytics" class="headerlink" title="Analytics"></a>Analytics</h1><h1 id="常用代码段整理合集"><a href="#常用代码段整理合集" class="headerlink" title="常用代码段整理合集"></a>常用代码段整理合集</h1><p>基础配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br></pre></td></tr></table></figure>

<p>检查 PyTorch 版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.__version__               <span class="comment"># PyTorch version</span></span><br><span class="line">torch.version.cuda              <span class="comment"># Corresponding CUDA version</span></span><br><span class="line">torch.backends.cudnn.version()  <span class="comment"># Corresponding cuDNN version</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)   <span class="comment"># GPU type</span></span><br></pre></td></tr></table></figure>

<p>更新 PyTorch<br>PyTorch 将被安装在 <code>anaconda3/lib/python3.7/site-packages/torch/</code>目录下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure>

<p>固定随机种子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">torch.cuda.manual_seed_all(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>指定程序运行在特定 GPU 卡上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在命令行指定环境变量</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python train.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#或在代码中指定</span></span><br><span class="line">os.environ[ CUDA_VISIBLE_DEVICES ] =  <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#判断是否有 CUDA 支持</span></span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>

<p>设置为 cuDNN benchmark 模式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Benchmark 模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果想要避免这种结果波动，设置</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>清除 GPU 存储</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有时 Control-C 中止运行后 GPU 存储没有及时释放，需要手动清空。在 PyTorch 内部可以</span></span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或在命令行可以先使用 ps 找到程序的 PID，再使用 kill 结束该进程</span></span><br><span class="line">ps aux | grep pythonkill <span class="number">-9</span> [pid]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接重置没有被清空的 GPU</span></span><br><span class="line">nvidia-smi --gpu-reset -i [gpu_id]</span><br></pre></td></tr></table></figure>

<p>张量处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 张量基本信息</span></span><br><span class="line">tensor.type()   <span class="comment"># Data type</span></span><br><span class="line">tensor.size()   <span class="comment"># Shape of the tensor. It is a subclass of Python tuple</span></span><br><span class="line">tensor.dim()    <span class="comment"># Number of dimensions.</span></span><br></pre></td></tr></table></figure>

<p>数据类型转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set default tensor type. Float in PyTorch is much faster than double.</span></span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Type convertions.</span></span><br><span class="line">tensor = tensor.cuda()</span><br><span class="line">tensor = tensor.cpu()</span><br><span class="line">tensor = tensor.float()</span><br><span class="line">tensor = tensor.long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Tensor 与 np.ndarray 转换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Tensor -&gt; np.ndarray.</span></span><br><span class="line">ndarray = tensor.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.ndarray -&gt; torch.Tensor.</span></span><br><span class="line">tensor = torch.from_numpy(ndarray).float()</span><br><span class="line">tensor = torch.from_numpy(ndarray.copy()).float()  <span class="comment"># If ndarray has negative stride</span></span><br></pre></td></tr></table></figure>

<p>torch.Tensor 与 PIL.Image 转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch 中的张量默认采用 N×D×H×W 的顺序，并且数据范围在 [0, 1]，需要进行转置和规范化。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Tensor -&gt; PIL.Image.</span></span><br><span class="line">image = PIL.Image.fromarray(torch.clamp(tensor * <span class="number">255</span>, min=<span class="number">0</span>, max=<span class="number">255</span></span><br><span class="line">    ).byte().permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).cpu().numpy())</span><br><span class="line">image = torchvision.transforms.functional.to_pil_image(tensor)  <span class="comment"># Equivalently way</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL.Image -&gt; torch.Tensor.</span></span><br><span class="line">tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))</span><br><span class="line">    ).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).float() / <span class="number">255</span></span><br><span class="line">tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))  <span class="comment"># Equivalently way</span></span><br></pre></td></tr></table></figure>

<p>np.ndarray 与 PIL.Image 转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.ndarray -&gt; PIL.Image.</span></span><br><span class="line">image = PIL.Image.fromarray(ndarray.astypde(np.uint8))</span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL.Image -&gt; np.ndarray.</span></span><br><span class="line">ndarray = np.asarray(PIL.Image.open(path))</span><br></pre></td></tr></table></figure>

<p>从只包含一个元素的张量中提取值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这在训练时统计 loss 的变化过程中特别有用。否则这将累积计算图，使 GPU 存储占用量越来越大。</span></span><br><span class="line">value = tensor.item()</span><br></pre></td></tr></table></figure>

<p>张量形变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">张量形变常常需要用于将卷积层特征输入全连接层的情形。相比 torch.view，torch.reshape 可以自动处理输入张量不连续的情况。</span><br><span class="line">tensor = torch.reshape(tensor, shape)</span><br></pre></td></tr></table></figure>

<p>打乱顺序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor[torch.randperm(tensor.size(<span class="number">0</span>))]  <span class="comment"># Shuffle the first dimension</span></span><br></pre></td></tr></table></figure>

<p>水平翻转</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch 不支持 tensor[::-1] 这样的负步长操作，水平翻转可以用张量索引实现。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume tensor has shape N*D*H*W.tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()]</span></span><br></pre></td></tr></table></figure>

<p>【复制张量】<br>有三种复制的方式，对应不同的需求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Operation                 |  New/Shared memory | Still in computation graph |</span></span><br><span class="line">tensor.clone()            <span class="comment"># |        New         |          Yes               |</span></span><br><span class="line">tensor.detach()           <span class="comment"># |      Shared        |          No                |</span></span><br><span class="line">tensor.detach.clone()()   <span class="comment"># |        New         |          No                |</span></span><br></pre></td></tr></table></figure>

<p>【拼接张量】<br>注意 torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，而 torch.stack 会新增一维。例如当参数是 3 个 10×5 的张量，torch.cat 的结果是 30×5 的张量，而 torch.stack 的结果是 3×10×5 的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.cat(list_of_tensors, dim=<span class="number">0</span>)</span><br><span class="line">tensor = torch.stack(list_of_tensors, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>将整数标记转换成独热（one-hot）编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch 中的标记默认从 0 开始。</span></span><br><span class="line">N = tensor.size(<span class="number">0</span>)</span><br><span class="line">one_hot = torch.zeros(N, num_classes).long()</span><br><span class="line">one_hot.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(tensor, dim=<span class="number">1</span>), src=torch.ones(N, num_classes).long())</span><br></pre></td></tr></table></figure>

<p>得到非零/零元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(tensor)               <span class="comment"># Index of non-zero elements</span></span><br><span class="line">torch.nonzero(tensor == <span class="number">0</span>)          <span class="comment"># Index of zero elements</span></span><br><span class="line">torch.nonzero(tensor).size(<span class="number">0</span>)       <span class="comment"># Number of non-zero elements</span></span><br><span class="line">torch.nonzero(tensor == <span class="number">0</span>).size(<span class="number">0</span>)  <span class="comment"># Number of zero elements</span></span><br></pre></td></tr></table></figure>

<p>张量扩展</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span></span><br><span class="line">torch.reshape(tensor, (<span class="number">64</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>)).expand(<span class="number">64</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<p>矩阵乘法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Matrix multiplication: (m*n) * (n*p) -&gt; (m*p).</span></span><br><span class="line">result = torch.mm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p).</span></span><br><span class="line">result = torch.bmm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Element-wise multiplication.</span></span><br><span class="line">result = tensor1 * tensor2</span><br></pre></td></tr></table></figure>

<p>计算两组数据之间的两两欧式距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X1 is of shape m*d.</span></span><br><span class="line">X1 = torch.unsqueeze(X1, dim=<span class="number">1</span>).expand(m, n, d)</span><br><span class="line"><span class="comment"># X2 is of shape n*d.</span></span><br><span class="line">X2 = torch.unsqueeze(X2, dim=<span class="number">0</span>).expand(m, n, d)</span><br><span class="line"><span class="comment"># dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2)</span></span><br><span class="line">dist = torch.sqrt(torch.sum((X1 - X2) ** <span class="number">2</span>, dim=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>模型定义</p>
<p>卷积层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最常用的卷积层配置是</span></span><br><span class="line">conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>如果卷积层配置比较复杂，不方便计算输出大小时，可以利用如下可视化工具辅助</p>
<p>链接：<a href="https://ezyang.github.io/convolution-visualizer/index.html" target="_blank" rel="noopener">https://ezyang.github.io/convolution-visualizer/index.html</a></p>
<p>GAP（Global average pooling）层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gap = torch.nn.AdaptiveAvgPool2d(output_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>双线性汇合（bilinear pooling）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.reshape(N, D, H * W)                        <span class="comment"># Assume X has shape N*D*H*W</span></span><br><span class="line">X = torch.bmm(X, torch.transpose(X, <span class="number">1</span>, <span class="number">2</span>)) / (H * W)  <span class="comment"># Bilinear pooling</span></span><br><span class="line"><span class="keyword">assert</span> X.size() == (N, D, D)</span><br><span class="line">X = torch.reshape(X, (N, D * D))</span><br><span class="line">X = torch.sign(X) * torch.sqrt(torch.abs(X) + <span class="number">1e-5</span>)   <span class="comment"># Signed-sqrt normalization</span></span><br><span class="line">X = torch.nn.functional.normalize(X)                  <span class="comment"># L2 normalization</span></span><br></pre></td></tr></table></figure>

<p>[多卡同步 BN（Batch normalization）]</p>
<ul>
<li>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差</li>
<li>同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差</li>
<li>缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</li>
<li>链接：<a href="https://github.com/vacancy/Synchronized-BatchNorm-PyTorch" target="_blank" rel="noopener">https://github.com/vacancy/Synchronized-BatchNorm-PyTorch</a></li>
</ul>
<p>[类似 BN 滑动平均]</p>
<ul>
<li>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BN</span><span class="params">(torch.nn.Module)</span></span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.register_buffer( running_mean , torch.zeros(num_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.running_mean += momentum * (current - self.running_mean)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>[计算模型整体参数量]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_parameters = sum(torch.numel(parameter) <span class="keyword">for</span> parameter <span class="keyword">in</span> model.parameters())</span><br></pre></td></tr></table></figure>

<p>[类似 Keras 的 model.summary() 输出模型信息]</p>
<ul>
<li>链接：<a href="https://github.com/sksq96/pytorch-summary" target="_blank" rel="noopener">https://github.com/sksq96/pytorch-summary</a></li>
</ul>
<p>[模型权值初始化</p>
<ul>
<li>注意 model.modules() 和 model.children() 的区别<ul>
<li>model.modules() 会迭代地遍历模型的所有子层</li>
<li>而 model.children() 只会遍历模型下的一层。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Common practise for initialization.</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.modules():</span><br><span class="line">    <span class="keyword">if</span> isinstance(layer, torch.nn.Conv2d):</span><br><span class="line">        torch.nn.init.kaiming_normal_(layer.weight, mode= fan_out ,</span><br><span class="line">                                      nonlinearity= relu )</span><br><span class="line">        <span class="keyword">if</span> layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> isinstance(layer, torch.nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.constant_(layer.weight, val=<span class="number">1.0</span>)</span><br><span class="line">        torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> isinstance(layer, torch.nn.Linear):</span><br><span class="line">        torch.nn.init.xavier_normal_(layer.weight)</span><br><span class="line">        <span class="keyword">if</span> layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialization with given tensor.</span></span><br><span class="line">layer.weight = torch.nn.Parameter(tensor)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>部分层使用预训练模型</p>
<ul>
<li>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load( model,pth ), strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>将在 GPU 保存的模型加载到 CPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load( model,pth , map_location= cpu ))</span><br></pre></td></tr></table></figure>

<h2 id="数据准备、特征提取与微调"><a href="#数据准备、特征提取与微调" class="headerlink" title="数据准备、特征提取与微调"></a>数据准备、特征提取与微调</h2><p>得到视频数据基本信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">video = cv2.VideoCapture(mp4_path)</span><br><span class="line">height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line">width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))</span><br><span class="line">num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="line">fps = int(video.get(cv2.CAP_PROP_FPS))</span><br><span class="line">video.release()</span><br></pre></td></tr></table></figure>

<p>TSN 每段（segment）采样一帧视频</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">K = self._num_segments</span><br><span class="line"><span class="keyword">if</span> is_train:</span><br><span class="line">    <span class="keyword">if</span> num_frames &gt; K:</span><br><span class="line">        <span class="comment"># Random index for each segment.</span></span><br><span class="line">        frame_indices = torch.randint(</span><br><span class="line">            high=num_frames // K, size=(K,), dtype=torch.long)</span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        frame_indices = torch.randint(</span><br><span class="line">            high=num_frames, size=(K - num_frames,), dtype=torch.long)</span><br><span class="line">        frame_indices = torch.sort(torch.cat((</span><br><span class="line">            torch.arange(num_frames), frame_indices)))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> num_frames &gt; K:</span><br><span class="line">        <span class="comment"># Middle index for each segment.</span></span><br><span class="line">        frame_indices = num_frames / K // <span class="number">2</span></span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        frame_indices = torch.sort(torch.cat((                              </span><br><span class="line">            torch.arange(num_frames), torch.arange(K - num_frames))))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> frame_indices.size() == (K,)</span><br><span class="line"><span class="keyword">return</span> [frame_indices[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(K)]</span><br></pre></td></tr></table></figure>

<p>提取 ImageNet 预训练模型某层的卷积特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VGG-16 relu5-3 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="literal">True</span>).features[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># VGG-16 pool5 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="literal">True</span>).features</span><br><span class="line"><span class="comment"># VGG-16 fc7 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:<span class="number">-3</span>])</span><br><span class="line"><span class="comment"># ResNet GAP feature.</span></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">model = torch.nn.Sequential(collections.OrderedDict(</span><br><span class="line">    list(model.named_children())[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.eval()</span><br><span class="line">    conv_representation = model(image)</span><br></pre></td></tr></table></figure>

<p>提取 ImageNet 预训练模型多层的卷积特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Helper class to extract several convolution features from the given</span></span><br><span class="line"><span class="string">    pre-trained model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        _model, torch.nn.Module.</span></span><br><span class="line"><span class="string">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(</span></span><br><span class="line"><span class="string">                list(model.named_children())[:-1]))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; conv_representation = FeatureExtractor(</span></span><br><span class="line"><span class="string">                pretrained_model=model,</span></span><br><span class="line"><span class="string">                layers_to_extract=&#123; layer1 ,  layer2 ,  layer3 ,  layer4 &#125;)(image)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrained_model, layers_to_extract)</span>:</span></span><br><span class="line">        torch.nn.Module.__init__(self)</span><br><span class="line">        self._model = pretrained_model</span><br><span class="line">        self._model.eval()</span><br><span class="line">        self._layers_to_extract = set(layers_to_extract)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            conv_representation = []</span><br><span class="line">            <span class="keyword">for</span> name, layer <span class="keyword">in</span> self._model.named_children():</span><br><span class="line">                x = layer(x)</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">in</span> self._layers_to_extract:</span><br><span class="line">                    conv_representation.append(x)</span><br><span class="line">            <span class="keyword">return</span> conv_representation</span><br></pre></td></tr></table></figure>

<p>其他预训练模型</p>
<ul>
<li>链接：<a href="https://github.com/Cadene/pretrained-models.pytorch" target="_blank" rel="noopener">https://github.com/Cadene/pretrained-models.pytorch</a></li>
</ul>
<p>微调全连接层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)  <span class="comment"># Replace the last fc layer</span></span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<p>以较大学习率微调全连接层，较小学习率微调卷积层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> id(p) <span class="keyword">not</span> <span class="keyword">in</span> finetuned_parameters)</span><br><span class="line">parameters = [&#123; params : conv_parameters,  lr : <span class="number">1e-3</span>&#125;, </span><br><span class="line">              &#123; params : model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>常用训练和验证数据预处理</p>
<ul>
<li>其中 ToTensor 操作会将 PIL.Image 或形状为 <code>H×W×D</code>，</li>
<li>数值范围为 [0, 255] 的 np.ndarray 转换为形状为 <code>D×H×W</code>，</li>
<li>数值范围为 [0.0, 1.0] 的 torch.Tensor。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(size=<span class="number">224</span>,</span><br><span class="line">                                             scale=(<span class="number">0.08</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize(mean=(<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>),</span><br><span class="line">                                     std=(<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)),</span><br><span class="line"> ])</span><br><span class="line"> val_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize(mean=(<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>),</span><br><span class="line">                                     std=(<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>训练基本代码框架</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> epoch(<span class="number">80</span>):</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> tqdm.tqdm(train_loader, desc= Epoch %<span class="number">3</span>d  % (t + <span class="number">1</span>)):</span><br><span class="line">        images, labels = images.cuda(), labels.cuda()</span><br><span class="line">        scores = model(images)</span><br><span class="line">        loss = loss_function(scores, labels)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<p>标记平滑（label smoothing）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line">    N = labels.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># C is the number of classes.</span></span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=<span class="number">0.1</span> / (C - <span class="number">1</span>)).cuda()</span><br><span class="line">    smoothed_labels.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(labels, dim=<span class="number">1</span>), value=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    score = model(images)</span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=<span class="number">1</span>)</span><br><span class="line">    loss = -torch.sum(log_prob * smoothed_labels) / N</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>Mixup</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup images.</span></span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()</span><br><span class="line">    index = torch.randperm(images.size(<span class="number">0</span>)).cuda()</span><br><span class="line">    mixed_images = lambda_ * images + (<span class="number">1</span> - lambda_) * images[index, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup loss.    </span></span><br><span class="line">    scores = model(mixed_images)</span><br><span class="line">    loss = (lambda_ * loss_function(scores, labels) </span><br><span class="line">            + (<span class="number">1</span> - lambda_) * loss_function(scores, labels[index]))</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>L1 正则化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction= sum )</span><br><span class="line">loss = ...  <span class="comment"># Standard cross-entropy loss</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>不对偏置项进行 L2 正则化/权值衰减（weight decay）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] ==  bias )</span><br><span class="line">others_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] !=  bias )</span><br><span class="line">parameters = [&#123; parameters : bias_list,  weight_decay : <span class="number">0</span>&#125;,                </span><br><span class="line">              &#123; parameters : others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<p>梯度裁剪（gradient clipping）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>计算 Softmax 输出的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">score = model(images)</span><br><span class="line">prediction = torch.argmax(score, dim=<span class="number">1</span>)</span><br><span class="line">num_correct = torch.sum(prediction == labels).item()</span><br><span class="line">accuruacy = num_correct / labels.size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>可视化模型前馈的计算图</p>
<ul>
<li>链接：<a href="https://github.com/szagoruyko/pytorchviz" target="_blank" rel="noopener">https://github.com/szagoruyko/pytorchviz</a></li>
</ul>
<p>可视化学习曲线</p>
<ul>
<li>有 Facebook 自己开发的 Visdom 和 Tensorboard 两个选择。</li>
<li><a href="https://github.com/facebookresearch/visdom" target="_blank" rel="noopener">https://github.com/facebookresearch/visdom</a></li>
<li><a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example using Visdom.</span></span><br><span class="line">vis = visdom.Visdom(env= Learning curve , use_incoming_socket=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">assert</span> self._visdom.check_connection()</span><br><span class="line">self._visdom.close()</span><br><span class="line">options = collections.namedtuple( Options , [ loss ,  acc ,  lr ])(</span><br><span class="line">    loss=&#123; xlabel :  Epoch ,  ylabel :  Loss ,  showlegend : <span class="literal">True</span>&#125;,</span><br><span class="line">    acc=&#123; xlabel :  Epoch ,  ylabel :  Accuracy ,  showlegend : <span class="literal">True</span>&#125;,</span><br><span class="line">    lr=&#123; xlabel :  Epoch ,  ylabel :  Learning rate ,  showlegend : <span class="literal">True</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> epoch(<span class="number">80</span>):</span><br><span class="line">    tran(...)</span><br><span class="line">    val(...)</span><br><span class="line">    vis.line(X=torch.Tensor([t + <span class="number">1</span>]), Y=torch.Tensor([train_loss]),</span><br><span class="line">             name= train , win= Loss , update= append , opts=options.loss)</span><br><span class="line">    vis.line(X=torch.Tensor([t + <span class="number">1</span>]), Y=torch.Tensor([val_loss]),</span><br><span class="line">             name= val , win= Loss , update= append , opts=options.loss)</span><br><span class="line">    vis.line(X=torch.Tensor([t + <span class="number">1</span>]), Y=torch.Tensor([train_acc]),</span><br><span class="line">             name= train , win= Accuracy , update= append , opts=options.acc)</span><br><span class="line">    vis.line(X=torch.Tensor([t + <span class="number">1</span>]), Y=torch.Tensor([val_acc]),</span><br><span class="line">             name= val , win= Accuracy , update= append , opts=options.acc)</span><br><span class="line">    vis.line(X=torch.Tensor([t + <span class="number">1</span>]), Y=torch.Tensor([lr]),</span><br><span class="line">             win= Learning rate , update= append , opts=options.lr)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>得到当前学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If there is one global learning rate (which is the common case).</span></span><br><span class="line">lr = next(iter(optimizer.param_groups))[ lr ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If there are multiple learning rates for different layers.</span></span><br><span class="line">all_lr = []</span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    all_lr.append(param_group[ lr ])</span><br></pre></td></tr></table></figure>

<p>学习率衰减</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reduce learning rate when validation accuarcy plateau.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= max , patience=<span class="number">5</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    train(...); val(...)</span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cosine annealing learning rate.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="number">80</span>)</span><br><span class="line"><span class="comment"># Reduce learning rate by 10 at given epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">50</span>, <span class="number">70</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...); val(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate warmup by 10 epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> t: t / <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...); val(...)</span><br></pre></td></tr></table></figure>

<p>保存与加载断点</p>
<ul>
<li>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save checkpoint.</span></span><br><span class="line">is_best = current_acc &gt; best_acc</span><br><span class="line">best_acc = max(best_acc, current_acc)</span><br><span class="line">checkpoint = &#123;</span><br><span class="line">     best_acc : best_acc,    </span><br><span class="line">     epoch : t + <span class="number">1</span>,</span><br><span class="line">     model : model.state_dict(),</span><br><span class="line">     optimizer : optimizer.state_dict(),</span><br><span class="line">&#125;</span><br><span class="line">model_path = os.path.join( model ,  checkpoint.pth.tar )</span><br><span class="line">torch.save(checkpoint, model_path)</span><br><span class="line"><span class="keyword">if</span> is_best:</span><br><span class="line">    shutil.copy( checkpoint.pth.tar , model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load checkpoint.</span></span><br><span class="line"><span class="keyword">if</span> resume:</span><br><span class="line">    model_path = os.path.join( model ,  checkpoint.pth.tar )</span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(model_path)</span><br><span class="line">    checkpoint = torch.load(model_path)</span><br><span class="line">    best_acc = checkpoint[ best_acc ]</span><br><span class="line">    start_epoch = checkpoint[ epoch ]</span><br><span class="line">    model.load_state_dict(checkpoint[ model ])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[ optimizer ])</span><br><span class="line">    print( Load checkpoint at epoch %d.  % start_epoch)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>计算准确率、查准率（precision）、查全率（recall）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data[ label ] and data[ prediction ] are groundtruth label and prediction </span></span><br><span class="line"><span class="comment"># for each image, respectively.</span></span><br><span class="line">accuracy = np.mean(data[ label ] == data[ prediction ]) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute recision and recall for each class.</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(len(num_classes)):</span><br><span class="line">    tp = np.dot((data[ label ] == c).astype(int),</span><br><span class="line">                (data[ prediction ] == c).astype(int))</span><br><span class="line">    tp_fp = np.sum(data[ prediction ] == c)</span><br><span class="line">    tp_fn = np.sum(data[ label ] == c)</span><br><span class="line">    precision = tp / tp_fp * <span class="number">100</span></span><br><span class="line">    recall = tp / tp_fn * <span class="number">100</span></span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-其他注意事项"><a href="#PyTorch-其他注意事项" class="headerlink" title="PyTorch 其他注意事项"></a>PyTorch 其他注意事项</h2><p>模型定义</p>
<ul>
<li><p>建议有参数的层和汇合（pooling）层使用 torch.nn 模块定义，激活函数直接使用 torch.nn.functional。torch.nn 模块和 torch.nn.functional 的区别在于，torch.nn 模块在计算时底层调用了 torch.nn.functional，但 torch.nn 模块包括该层参数，还可以应对训练和测试两种网络状态。使用 torch.nn.functional 时要注意网络状态，如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    x = torch.nn.functional.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br></pre></td></tr></table></figure>
</li>
<li><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p>
</li>
<li><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和随机失活（dropout）在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</p>
</li>
<li><p><code>torch.nn.CrossEntropyLoss</code> 的输入不需要经过 Softmax。<code>torch.nn.CrossEntropyLoss</code> 等价于 <code>torch.nn.functional.log_softmax + torch.nn.NLLLoss</code>。</p>
</li>
<li><p><code>loss.backward()</code> 前用 <code>optimizer.zero_grad()</code> 清除累积梯度。<code>optimizer.zero_grad()</code> 和 <code>model.zero_grad()</code> 效果一样。</p>
</li>
</ul>
<p>PyTorch 性能与调试</p>
<ul>
<li><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。<code>num_workers</code>的设置需要在实验中找到最快的取值。</li>
<li>用 del 及时删除不用的中间变量，节约 GPU 存储。</li>
<li>使用 inplace 操作可节约 GPU 存储，如<ul>
<li><code>x = torch.nn.functional.relu(x, inplace=True)</code></li>
</ul>
</li>
<li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li>
<li>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li>
<li>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</li>
<li>除了标记 y 外，尽量少使用一维张量，使用 <code>n*1</code> 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li>
<li>统计代码各部分耗时<ul>
<li><code>with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:</code></li>
<li><code>...</code></li>
<li><code>print(profile)</code></li>
</ul>
</li>
<li>或者在命令行运行<ul>
<li><code>python -m torch.utils.bottleneck main.py</code></li>
</ul>
</li>
</ul>
<h1 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h1><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h2 id="使用-TensorBoard"><a href="#使用-TensorBoard" class="headerlink" title="使用 TensorBoard"></a>使用 TensorBoard</h2><h3 id="多次试验比较超参数"><a href="#多次试验比较超参数" class="headerlink" title="多次试验比较超参数"></a>多次试验比较超参数</h3><p>一个代码实例：有两个超参数需要选择，可以使用两层 for loop</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">batch_size_list = [<span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>]</span><br><span class="line">lr_list = [<span class="number">.01</span>, <span class="number">.001</span>, <span class="number">.0001</span>, <span class="number">.00001</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_size <span class="keyword">in</span> batch_size_list:</span><br><span class="line">    <span class="keyword">for</span> lr <span class="keyword">in</span> lr_list:</span><br><span class="line">        network = Network()</span><br><span class="line"></span><br><span class="line">        train_loader = torch.utils.data.DataLoader(</span><br><span class="line">            train_set, batch_size=batch_size</span><br><span class="line">        )</span><br><span class="line">        optimizer = optim.Adam(</span><br><span class="line">            network.parameters(), lr=lr</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        images, labels = next(iter(train_loader))</span><br><span class="line">        grid = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line">        comment=<span class="string">f' batch_size=<span class="subst">&#123;batch_size&#125;</span> lr=<span class="subst">&#123;lr&#125;</span>'</span></span><br><span class="line">        tb = SummaryWriter(comment=comment)</span><br><span class="line">        tb.add_image(<span class="string">'images'</span>, grid)</span><br><span class="line">        tb.add_graph(network, images)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">                images, labels = batch    <span class="comment"># Get Batch</span></span><br><span class="line">                preds = network(images)    <span class="comment"># Pass Batch</span></span><br><span class="line">                loss = F.cross_entropy(preds, labels)    <span class="comment"># Calculate Loss</span></span><br><span class="line">                optimizer.zero_grad()    <span class="comment"># Zero Gradients</span></span><br><span class="line">                loss.backward()    <span class="comment"># Calculate Gradients</span></span><br><span class="line">                optimizer.step()    <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">                total_loss += loss.item() * batch_size</span><br><span class="line">                total_correct += get_num_correct(preds, labels)</span><br><span class="line"></span><br><span class="line">            tb.add_scalar(</span><br><span class="line">                <span class="string">'Loss'</span>, total_loss, epoch</span><br><span class="line">            )</span><br><span class="line">            tb.add_scalar(</span><br><span class="line">                <span class="string">'Number Correct'</span>, total_correct, epoch</span><br><span class="line">            )</span><br><span class="line">            tb.add_scalar(</span><br><span class="line">                <span class="string">'Accuracy'</span>, total_correct / len(train_set), epoch</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> network.named_parameters():</span><br><span class="line">                tb.add_histogram(name, param, epoch)</span><br><span class="line">                tb.add_histogram(<span class="string">f'<span class="subst">&#123;name&#125;</span>.grad'</span>, param.grad, epoch)</span><br><span class="line"></span><br><span class="line">            print(</span><br><span class="line">                <span class="string">"epoch"</span>, epoch</span><br><span class="line">                ,<span class="string">"total_correct:"</span>, total_correct</span><br><span class="line">                ,<span class="string">"loss:"</span>, total_loss</span><br><span class="line">            )  </span><br><span class="line">        tb.close()</span><br></pre></td></tr></table></figure>


<p>如果有更多的超参数，此时使用很多个 for loop 会带来很大的缩进从而让代码变得不太好看！可以使用如下的技巧。</p>
<ol>
<li>If we have a list of parameters, we can package them up into a set for each of our runs using the Cartesian product. For this we’ll use the product function from the itertools library.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, we define a dictionary that contains parameters as keys and parameter values we want to use as values.</span></span><br><span class="line"></span><br><span class="line">parameters = dict(</span><br><span class="line">    lr = [<span class="number">.01</span>, <span class="number">.001</span>]</span><br><span class="line">    ,batch_size = [<span class="number">100</span>, <span class="number">1000</span>]</span><br><span class="line">    ,shuffle = [<span class="literal">True</span>, <span class="literal">False</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, we'll create a list of iterables that we can pass to the product functions.</span></span><br><span class="line"></span><br><span class="line">param_values = [v <span class="keyword">for</span> v <span class="keyword">in</span> parameters.values()]</span><br><span class="line">param_values    <span class="comment"># [[0.01, 0.001], [100, 1000], [True, False]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, we have three lists of parameter values. After we take the Cartesian product of these three lists, we'll have a set of parameter values for each of our runs. Note that this is equivalent to nested for-loops, as the doc string of the product function indicates.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr, batch_size, shuffle <span class="keyword">in</span> product(*param_values): </span><br><span class="line">    <span class="keyword">print</span> (lr, batch_size, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.01 100 True</span></span><br><span class="line"><span class="comment"># 0.01 100 False</span></span><br><span class="line"><span class="comment"># 0.01 1000 True</span></span><br><span class="line"><span class="comment"># 0.01 1000 False</span></span><br><span class="line"><span class="comment"># 0.001 100 True</span></span><br><span class="line"><span class="comment"># 0.001 100 False</span></span><br><span class="line"><span class="comment"># 0.001 1000 True</span></span><br><span class="line"><span class="comment"># 0.001 1000 False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr, batch_size, shuffle <span class="keyword">in</span> product(*param_values): </span><br><span class="line">    comment = <span class="string">f' batch_size=<span class="subst">&#123;batch_size&#125;</span> lr=<span class="subst">&#123;lr&#125;</span> shuffle=<span class="subst">&#123;shuffle&#125;</span>'</span></span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        train_set</span><br><span class="line">        ,batch_size=batch_size</span><br><span class="line">        ,shuffle=shuffle </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = optim.Adam(</span><br><span class="line">        network.parameters(), lr=lr</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Rest of training process given the set of parameters</span></span><br></pre></td></tr></table></figure>

<p>Note the way we build our comment string to identify the run. We just plug in the values. Also, notice the <code>*</code> operator. This is a special way in Python to unpack a list into a set of arguments. Thus, in this situation, we have passing three individual unpacked arguments to the <code>product</code> function opposed to the single list.</p>
<h3 id="可视化-network-parameters"><a href="#可视化-network-parameters" class="headerlink" title="可视化 network parameters"></a>可视化 network parameters</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, weight <span class="keyword">in</span> network.named_parameters():</span><br><span class="line">    tb.add_histogram(name, weight, epoch)</span><br><span class="line">    tb.add_histogram(<span class="string">f'<span class="subst">&#123;name&#125;</span>.grad'</span>, weight.grad, epoch)</span><br></pre></td></tr></table></figure>

<p>This works because the PyTorch <code>nn.Module</code> method called <code>named_parameters()</code> gives us the name and value of all the parameters inside the network.</p>
<h1 id="高级功能"><a href="#高级功能" class="headerlink" title="高级功能"></a>高级功能</h1><h2 id="RunBuilder-类"><a href="#RunBuilder-类" class="headerlink" title="RunBuilder 类"></a>RunBuilder 类</h2><p>先看一下长啥样，后面在分析为什么能工作！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RunBuilder</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_runs</span><span class="params">(params)</span>:</span></span><br><span class="line"></span><br><span class="line">        Run = namedtuple(<span class="string">'Run'</span>, params.keys())</span><br><span class="line"></span><br><span class="line">        runs = []</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> product(*params.values()):</span><br><span class="line">            runs.append(Run(*v))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> runs</span><br></pre></td></tr></table></figure>

<p>The main thing to note about using this class is that it has a <font color='red'>static</font> method called <font color='red'>get_runs()</font>. This method will get the runs for us that it builds based on the parameters we pass in. Since the <font color='red'>get_runs()</font> method is static, we can call it using the class itself. We don’t need an instance of the class.</p>
<p>之后，代码编程如下形式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">params = OrderedDict(</span><br><span class="line">    lr = [<span class="number">.01</span>, <span class="number">.001</span>]</span><br><span class="line">    ,batch_size = [<span class="number">1000</span>, <span class="number">10000</span>]</span><br><span class="line">    ,device = [<span class="string">"cuda"</span>, <span class="string">"cpu"</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Before:</span></span><br><span class="line"><span class="keyword">for</span> lr, batch_size, shuffle <span class="keyword">in</span> product(*param_values):</span><br><span class="line">    comment = <span class="string">f' batch_size=<span class="subst">&#123;batch_size&#125;</span> lr=<span class="subst">&#123;lr&#125;</span> shuffle=<span class="subst">&#123;shuffle&#125;</span>'</span></span><br><span class="line">    <span class="comment"># Training process given the set of parameters</span></span><br><span class="line"></span><br><span class="line">After:</span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> RunBuilder.get_runs(params):</span><br><span class="line">    comment = <span class="string">f'-<span class="subst">&#123;run&#125;</span>'</span></span><br><span class="line">    <span class="comment"># Training process given the set of parameters</span></span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-md/" rel="next" title="计算机网络.md">
                <i class="fa fa-chevron-left"></i> 计算机网络.md
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/10/19/python-data-etl-codes-md/" rel="prev" title="python-data-etl-codes.md">
                python-data-etl-codes.md <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Zheng Xing" />
            
              <p class="site-author-name" itemprop="name">Zheng Xing</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">36</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Intro"><span class="nav-number">1.</span> <span class="nav-text">Intro</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensor-操作"><span class="nav-number">2.</span> <span class="nav-text">Tensor 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建"><span class="nav-number">2.1.</span> <span class="nav-text">创建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分析性质"><span class="nav-number">2.2.</span> <span class="nav-text">分析性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#比较"><span class="nav-number">2.3.</span> <span class="nav-text">比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#切片-start-end-step"><span class="nav-number">2.4.</span> <span class="nav-text">切片(start : end : step)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#索引查找"><span class="nav-number">2.5.</span> <span class="nav-text">索引查找</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#维度变换"><span class="nav-number">2.6.</span> <span class="nav-text">维度变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#view-reshape-flatten"><span class="nav-number">2.6.1.</span> <span class="nav-text">view&#x2F;reshape&#x2F;flatten</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#squeeze-unsqueeze"><span class="nav-number">2.6.2.</span> <span class="nav-text">squeeze&#x2F;unsqueeze</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#expand-repeat"><span class="nav-number">2.6.3.</span> <span class="nav-text">expand&#x2F;repeat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transpose-permute"><span class="nav-number">2.6.4.</span> <span class="nav-text">transpose&#x2F;permute</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Broadcasting"><span class="nav-number">2.7.</span> <span class="nav-text">Broadcasting:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拼接与拆分-cat-stack-spilit-chunk"><span class="nav-number">2.8.</span> <span class="nav-text">拼接与拆分(cat&#x2F;stack&#x2F;spilit&#x2F;chunk):</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cat-拼接"><span class="nav-number">2.8.1.</span> <span class="nav-text">cat 拼接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stack增维度拼接"><span class="nav-number">2.8.2.</span> <span class="nav-text">stack增维度拼接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#split拆分"><span class="nav-number">2.8.3.</span> <span class="nav-text">split拆分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chunk拆分"><span class="nav-number">2.8.4.</span> <span class="nav-text">chunk拆分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运算"><span class="nav-number">2.9.</span> <span class="nav-text">运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#统计属性"><span class="nav-number">2.10.</span> <span class="nav-text">统计属性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#where-gather"><span class="nav-number">2.11.</span> <span class="nav-text">where&#x2F;gather</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataSet-DataLoader"><span class="nav-number">3.</span> <span class="nav-text">DataSet&#x2F;DataLoader</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#torchvision"><span class="nav-number">3.1.</span> <span class="nav-text">torchvision</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataLoader"><span class="nav-number">3.2.</span> <span class="nav-text">DataLoader</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Models"><span class="nav-number">4.</span> <span class="nav-text">Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#继承-nn-Module-基类"><span class="nav-number">4.1.</span> <span class="nav-text">继承 nn.Module 基类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#parameters"><span class="nav-number">4.2.</span> <span class="nav-text">parameters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#forward-实现"><span class="nav-number">4.3.</span> <span class="nav-text">forward 实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training"><span class="nav-number">5.</span> <span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-single-Batch"><span class="nav-number">5.1.</span> <span class="nav-text">A single Batch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Many-epoch-of-many-batches"><span class="nav-number">5.2.</span> <span class="nav-text">Many epoch of many batches</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度"><span class="nav-number">5.3.</span> <span class="nav-text">梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-no-grad"><span class="nav-number">5.3.1.</span> <span class="nav-text">torch.no_grad()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-train"><span class="nav-number">5.4.</span> <span class="nav-text">model.train()</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Analytics"><span class="nav-number">6.</span> <span class="nav-text">Analytics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常用代码段整理合集"><span class="nav-number">7.</span> <span class="nav-text">常用代码段整理合集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据准备、特征提取与微调"><span class="nav-number">7.1.</span> <span class="nav-text">数据准备、特征提取与微调</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练"><span class="nav-number">7.2.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch-其他注意事项"><span class="nav-number">7.3.</span> <span class="nav-text">PyTorch 其他注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorBoard"><span class="nav-number">8.</span> <span class="nav-text">TensorBoard</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#配置"><span class="nav-number">8.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用-TensorBoard"><span class="nav-number">8.2.</span> <span class="nav-text">使用 TensorBoard</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多次试验比较超参数"><span class="nav-number">8.2.1.</span> <span class="nav-text">多次试验比较超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#可视化-network-parameters"><span class="nav-number">8.2.2.</span> <span class="nav-text">可视化 network parameters</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#高级功能"><span class="nav-number">9.</span> <span class="nav-text">高级功能</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RunBuilder-类"><span class="nav-number">9.1.</span> <span class="nav-text">RunBuilder 类</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zheng Xing</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://githubio-2.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2020/07/28/pytorch-summary/';
          this.page.identifier = '2020/07/28/pytorch-summary/';
          this.page.title = 'pytorch-summary';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://githubio-2.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  











<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
